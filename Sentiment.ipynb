{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk>=3.1 (from textblob)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 35.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk, textblob\n",
      "Successfully installed nltk-3.4.5 textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "review5 = \"The lint eater is amazing! We recently bought a house that was built in 2001 and previously owned by an older lady, so I assumed the dryer vent had never been cleaned. Before setting up the dryer, I wanted to give the vent line a thorough cleaning for safety's sake and to help the dryer run as efficiently as possible. Well, I'm glad I did! The exterior cover on the vent had been gone for some time, and the amount of leaves and pine needles plus lint wads that I got out were shocking! It seemed like every time I ran the LintEater through line, it dislodged another fist-sized wad of stuff. When I first hooked up the shop vac inside the house, and went out to the vent exit, there was almost zero air getting sucked in. The wads were so huge that they kept stopping up my shop vac hose and I had to keep going inside to clean it out. After all was said and done, my vent line is free-flowing and working as good as new. Glad I spent the money on this product!\\n\\nHelpful tip - when I started, I just jammed all of the rods together and tried to work it into the vent line. It's way too unwieldy this way, and every time you hit the power on the drill, the rods get kinked and knotted up because they're so flexible. Start with one, and when that one gets most of the way into the vent line, add another. Repeat as necessary and you'll have a MUCH easier time.\"\n",
    "review5 = \"I really like this pizza. It is amazing. Good to eat and very affordable. I would buy it again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.425, subjectivity=0.5)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "blob = TextBlob(review5)  #, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment\n",
    "# No cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really like pizza amazing good eat affordable would buy \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "preprocessed_words = \"\"\n",
    "words = blob.lower().tokenize()\n",
    "for word in words:\n",
    "    if word not in string.punctuation and word not in stopwords.words('english'):\n",
    "        word = word.lemmatize()\n",
    "        if not word.startswith(\"'\"):\n",
    "            preprocessed_words += word + \" \"\n",
    "            \n",
    "print(preprocessed_words)\n",
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.5666666666666668)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob2 = TextBlob(preprocessed_words)\n",
    "blob2.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
